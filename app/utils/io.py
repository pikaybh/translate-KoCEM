import base64, json, os, re, yaml
import numpy as np
from collections import defaultdict
import pandas as pd
from PIL import Image
from pandas_image_methods import PILMethods
import io

from schemas import Option, Quiz


# Flatten nested dicts/lists for Parquet compatibility
def serialize_for_parquet(obj):
    if isinstance(obj, Option):
        return obj.model_dump()
    elif isinstance(obj, Quiz):
        return obj.model_dump()
    elif isinstance(obj, list):
        if obj and isinstance(obj[0], Option):
            return [o.model_dump() for o in obj]
        elif obj and isinstance(obj[0], Quiz):
            return [q.model_dump() for q in obj]
    return obj


def extract_bytes(b):
    # dictë¡œ í•œ ë²ˆì´ë¼ë„ ê°ì‹¸ì ¸ ìˆìœ¼ë©´ ëê¹Œì§€ 'bytes'ë§Œ ì¶”ì¶œ
    while isinstance(b, dict):
        b = b.get('bytes', None)
    return b


# ëª¨ë“  ì»¬ëŸ¼ì„ dict/listëŠ” json ë¬¸ìì—´ë¡œ, ìˆ«ì íƒ€ì…ì€ stringìœ¼ë¡œ ë³€í™˜
def safe_json(obj):
    if isinstance(obj, bytes):  # ğŸ”¥ ì´ ì¤„ ì¶”ê°€!
        return obj
    
    if isinstance(obj, (Option, Quiz)):
        return obj.model_dump()
    
    elif isinstance(obj, list):
        # ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€: dict/objectëŠ” ê·¸ëŒ€ë¡œ, ë‚˜ë¨¸ì§€ëŠ” strë¡œ ë³€í™˜
        out = []
        for item in obj:
            if isinstance(item, dict):
                out.append(safe_json(item))
            elif isinstance(item, (Option, Quiz)):
                out.append(item.model_dump())
            else:
                out.append(str(item) if item is not None else "None")
        return out
    
    if isinstance(obj, dict):
        # image dict ë‚´ë¶€ë¼ë©´, bytes keyë§Œ ë”°ë¡œ ì˜ˆì™¸ ì²˜ë¦¬
        if set(obj.keys()) == {'bytes', 'path'}:
            return {
                'bytes': obj['bytes'] if isinstance(obj['bytes'], bytes) else extract_bytes(obj['bytes']),
                'path': obj['path']
            }
        return {k: safe_json(v) for k, v in obj.items()}
    
    elif isinstance(obj, str):
        return str(obj)
    
    elif isinstance(obj, int):
        return int(obj)
    elif isinstance(obj, float):
        return float(obj)
    # listê°€ ì•„ë‹Œë° listë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê²½ìš° (ì˜ˆ: np.ndarray, tuple ë“±)
    try:
        as_list = list(obj)
        # strì€ iterableì´ì§€ë§Œ listë¡œ ë³€í™˜í•˜ë©´ ë¬¸ì ë‹¨ìœ„ë¡œ ìª¼ê°œì§€ë¯€ë¡œ ì œì™¸
        if not isinstance(obj, str):
            return safe_json(as_list)
    except Exception:
        pass
    # ë³€í™˜ ë¶ˆê°€ë©´ strë¡œ ì €ì¥
    # return str(obj) if obj is not None else "None"
    return obj
    # raise ValueError("Unsupported type for safe_json conversion: {}: {}".format(obj, type(obj)))


def _display_img(img):
    import matplotlib.pyplot as plt
    import sys

    # Only display if running interactively
    if hasattr(sys, 'ps1') or sys.flags.interactive:
        plt.figure()
        plt.imshow(img)
        plt.axis('off')
        plt.show()


def _image_to_base64(x, display_image: bool = True) -> dict:
    """
    ì´ë¯¸ì§€ dict/bytesë¥¼ ë°›ì•„ {"bytes": bytes, "path": str (optional)}ë¡œ ë³€í™˜.
    display_image=Trueì¼ ê²½ìš° ë³€í™˜ëœ ì´ë¯¸ì§€ë¥¼ ë°”ë¡œ display(IPython)ë¡œ ë³´ì—¬ì¤Œ.
    output_format: 'base64' (default) or 'hex' (16ì§„ìˆ˜ ë¬¸ìì—´)
    """
    bytes_ = x.get('bytes', None)
    path_ = x.get('path', None)
    
    if bytes_:
        img = Image.open(io.BytesIO(bytes_))
        if display_image:
            _display_img(img)
        with io.BytesIO() as buf:
            img.save(buf, format=img.format or 'PNG')
            buf.seek(0)  # ë²„í¼ì˜ ì‹œì‘ìœ¼ë¡œ ì´ë™
            y = {
                "bytes": bytes(buf.getvalue()),
                "path": path_
            }
    else:
        y = {
            "bytes": bytes(),
            "path": path_
        }
    print("({}) {}...".format(type(y["bytes"]), y["bytes"][:10]))  # Print first 10 bytes for debug
    return y


def save_parquet(path: str, rows: list):
    """
    Parquet íŒŒì¼ì„ pandasë¡œ ì €ì¥í•©ë‹ˆë‹¤. (pyarrow ì™„ì „ ì œê±°)
    ëª¨ë“  dict/list ì»¬ëŸ¼ì€ JSON ë¬¸ìì—´ë¡œ ë³€í™˜, ì´ë¯¸ì§€ bytesëŠ” ê·¸ëŒ€ë¡œ ì €ì¥.
    """
    list_candidates = ["options", "eval_loop", "history", "feedbacks", "splits"]

    # Step 1: ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ JSON ë¬¸ìì—´ íŒŒì‹±
    def parse_json_list(val):
        if isinstance(val, str):
            s = val.strip()
            if s.startswith('[') and s.endswith(']'):
                parsed = json.loads(s)
                if isinstance(parsed, list):
                    return parsed
        return val

    for row in rows:
        for col in list_candidates:
            if col in row:
                row[col] = parse_json_list(row[col])

    for row in rows:
        if "image" in row:
            row["image"] = _image_to_base64(row["image"])
            print(f"ì´ë¯¸ì§€ í‚¤: {row['image'].keys()}")
            print(f"ì´ë¯¸ì§€ bytes: {row['image']['bytes'][:10]}...")
            print(f"ì´ë¯¸ì§€ path: {row['image']['path']}")

    # Step 2: ëª¨ë“  dict/list ì»¬ëŸ¼ì€ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
    def flatten_for_df(obj):
        if isinstance(obj, (dict, list)):
            return json.dumps(obj, ensure_ascii=False)
        return obj

    rows = [safe_json(row) for row in rows]
    for row in rows:
        for k, v in row.items():
            if isinstance(v, (dict, list)) and k != "image":
                row[k] = flatten_for_df(v)
            if k == "image" and isinstance(v, dict):
                # ì´ë¯¸ì§€ bytesëŠ” ê·¸ëŒ€ë¡œ, pathëŠ” string
                pass

    # Step 3: DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(rows)
    df.to_parquet(path, index=False)
    print(f"[ì™„ë£Œ] Parquet ì €ì¥: {path}, shape={df.shape}")


def refresh_terminal():
    # Terminal refresh (ctrl+l effect)
    print("\033[2J\033[1;1H", end="")


def write_readme_from_outputs(
    output_dir: str = "output",
    eval_result_dir: str = "eval_results",
    readme_path: str = "output/gpt-eval/README.md"
):
    """
    ë²ˆì—­ ê²°ê³¼ë¬¼(Parquet íŒŒì¼ë“¤)ë¡œë¶€í„° README.mdë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤. (pandas ê¸°ë°˜)
    """
    configs = defaultdict(lambda: {"splits": {}, "features": set()})
    all_tags = set()
    total_instances = 0

    eval_dir = os.path.join(output_dir, eval_result_dir)
    if not os.path.exists(eval_dir):
        raise FileNotFoundError(f"{eval_dir} ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # 1. configë³„ splitë³„ parquet íŒŒì¼ íƒìƒ‰ (pandas ê¸°ë°˜)
    for config_name in os.listdir(eval_dir):
        if not config_name == "Drawing_Interpretation":
            continue
        config_path = os.path.join(eval_dir, config_name)
        if not os.path.isdir(config_path):
            continue
        for f in os.listdir(config_path):
            if not f.endswith(".parquet"): continue
            split = f.split(".")[0]
            file_path = os.path.join(config_path, f)
            try:
                df = pd.read_parquet(file_path)
            except Exception:
                continue
            num_examples = len(df)
            total_instances += num_examples
            configs[config_name]["splits"][split] = {
                "num_examples": num_examples,
                "file": os.path.relpath(file_path, os.path.join(output_dir, eval_result_dir))
            }
            configs[config_name]["features"].update(df.columns)
            # íƒœê·¸ ì¶”ì¶œ (ì˜ˆ: question_type, field ë“±)
            for col in ["question_type", "field", "subfield"]:
                if col in df.columns:
                    tags = set(x for x in df[col].dropna().unique() if x is not None)
                    all_tags.update(tags)

    # 2. YAML í—¤ë” ìƒì„±
    yaml_lines = ["---"]
    yaml_lines.append("language:\n- ko\n- en")
    yaml_lines.append("license: cc-by-nc-4.0")
    yaml_lines.append("size_categories:\n- 10K<n<100K")
    yaml_lines.append("task_categories:\n- question-answering\n- multiple-choice")
    yaml_lines.append(f"pretty_name: kocem:{eval_result_dir.lower()}")
    yaml_lines.append("dataset_info:")

    def pandas_type_to_hf_feature(dtype):
        t = str(dtype)
        if t.startswith("int"):
            return "int64"
        elif t.startswith("float"):
            return "float64"
        elif t == "bool":
            return "bool"
        elif t == "object":
            return "string"
        else:
            return "string"

    for config, info in configs.items():
        yaml_lines.append(f"  {config}:")
        yaml_lines.append(f"    features:")
        dtype_map = {}
        schema_map = {}
        # splitë³„ë¡œ dtype ì¶”ì¶œ
        for s_info in info["splits"].values():
            split_file = s_info["file"]
            try:
                df = pd.read_parquet(os.path.join(output_dir, eval_result_dir, split_file))
                for col in df.columns:
                    dtype_map[col] = str(df[col].dtype)
                    schema_map[col] = df[col].dtype
            except Exception:
                print(f"[ì˜¤ë¥˜] Parquet íŒŒì¼ dtype ì¶”ì¶œ ì‹¤íŒ¨: {split_file}")
                pass
        # ë“¤ì—¬ì“°ê¸°: 4ì¹¸
        for feat in sorted(info["features"]):
            dtype = dtype_map.get(feat, None)
            if dtype is not None:
                yaml_lines.append(f"      - name: {feat}\n        dtype: {pandas_type_to_hf_feature(dtype)}")
            else:
                yaml_lines.append(f"      - name: {feat}\n        dtype: null")
        yaml_lines.append(f"    splits:")
        for split, s_info in info["splits"].items():
            yaml_lines.append(f"      - name: {split}\n        num_examples: {s_info['num_examples']}")
        feature_list = ', '.join(sorted(info["features"]))
        split_list = ', '.join(sorted(info["splits"].keys()))
        n = sum(s["num_examples"] for s in info["splits"].values())
        tag_list = ', '.join(sorted(all_tags)) if all_tags else "-"
        yaml_lines.append(f"    description: |\n      ì´ configëŠ” {split_list} splitì— ê±¸ì³ {n}ê°œì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n      Features: {feature_list}.\n      ì£¼ìš” íƒœê·¸: {tag_list}.")
    yaml_lines.append("configs:")
    for config, info in configs.items():
        yaml_lines.append(f"- config_name: {config}")
        yaml_lines.append("  data_files:")
        for split, s_info in info["splits"].items():
            yaml_lines.append(f"  - split: {split}")
            path = s_info['file'].replace('\\', '/').replace('\\', '/')
            yaml_lines.append(f"    path: {path}")
    yaml_lines.append("tags:")
    for tag in sorted(all_tags):
        yaml_lines.append(f"- {tag}")
    yaml_lines.append("---\n")

    # 3. ë³¸ë¬¸(í‘œ, ì„¤ëª… ë“±)
    table_lines = [
        f"\n# KoCEM:{eval_result_dir}\n",
        "| Config | Splits | Features | Instances |",
        "|--------|--------|----------|-----------|"
    ]
    for config, info in configs.items():
        splits = ", ".join(sorted(info["splits"].keys()))
        features = ", ".join(sorted(info["features"]))
        n = sum(s["num_examples"] for s in info["splits"].values())
        table_lines.append(f"| {config} | {splits} | {features} | {n} |")
    table_lines.append(f"| **Total** |  |  | **{total_instances}** |")

    disclaimer = (
        "\n<div style=\"border: 2px solid #f87171; background-color: #fef2f2; border-radius: 12px; padding: 20px; margin: 20px 0; color: #991b1b; font-family: 'Segoe UI', 'Apple SD Gothic Neo', Arial, sans-serif; font-size: 1.05em;\">\n"
        "  <strong style=\"font-size:1.18em;\">âš ï¸ Disclaimer</strong><br>\n"
        "  ë³¸ ë°ì´í„°ì…‹ì€ ê°œì¸ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ë…ë¦½ì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.<br>\n"
        "  ì–´ë– í•œ ê²½ìš°ì—ë„ ê³µìœ ê°€ ë¶ˆê°€í•¨ì„ ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤.<br>\n"
        "  ì ‘ê·¼ ìš”ì²­ì€ ëª¨ë‘ ê±°ì ˆë  ì˜ˆì •ì´ë‹ˆ, ìš”ì²­ì„ ì‚¼ê°€ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.<br>\n"
        "  <u>ì´í•´ì™€ ë°°ë ¤ì— ê¹Šì´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</u>\n"
        "</div>\n"
    )

    contact = "\n## ë¬¸ì˜\n\n- pikaybh@snu.ac.kr\n"

    # 4. íŒŒì¼ë¡œ ì €ì¥
    with open(readme_path, "w", encoding="utf-8") as f:
        f.write("\n".join(yaml_lines))
        f.write("\n".join(table_lines))
        f.write(disclaimer)
        f.write(contact)


__all__ = ['save_parquet', 'serialize_for_parquet', 'refresh_terminal', 'write_readme_from_outputs']